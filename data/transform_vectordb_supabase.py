import psycopg2
from dotenv import load_dotenv
from tqdm import tqdm
import os
import pickle
from datetime import datetime

load_dotenv()

EMB_DIR = "data/embeddings"
LOCAL_DB_URL = os.getenv("POSTGRES_URL")
TABLE_NAME = "whoisme.embeddings"

# --- K·∫øt n·ªëi PostgreSQL local ---
conn = psycopg2.connect(LOCAL_DB_URL)
cursor = conn.cursor()
print("ƒê√£ k·∫øt n·ªëi t·ªõi PostgreSQL local th√†nh c√¥ng.")


def load_vector_files():
    return [f for f in os.listdir(EMB_DIR) if f.endswith(".pkl")]


def upload_embeddings():
    files = load_vector_files()
    if not files:
        print("Kh√¥ng c√≥ file embedding n√†o trong th∆∞ m·ª•c data/embeddings.")
        return

    for file in files:
        path = os.path.join(EMB_DIR, file)
        with open(path, "rb") as f:
            data = pickle.load(f)

        sheet_name = data.get("sheet_name", "unknown")
        embeddings_by_col = data.get("embeddings_by_col", {})
        df = data.get("df")
        data_hash = data.get("data_hash", "")
        updated_at = data.get("updated_at", datetime.now().isoformat())

        rows = []

        # Duy·ªát t·ª´ng c·ªôt embedding
        for col_name, col_data in embeddings_by_col.items():
            if isinstance(col_data, dict):
                texts = col_data.get("texts", [])
                embs = col_data.get("embeddings", [])
            else:
                embs = col_data
                texts = (
                    df[col_name].astype(str).tolist()
                    if df is not None and col_name in df.columns
                    else []
                )

            if len(texts) != len(embs):
                print(f"S·ªë l∆∞·ª£ng text v√† embedding kh√¥ng kh·ªõp trong c·ªôt {col_name}")
                continue

            for idx, (text, emb) in enumerate(zip(texts, embs)):
                if hasattr(emb, "tolist"):
                    emb = emb.tolist()

                metadata = {}
                level = None
                if df is not None and idx < len(df):
                    metadata = df.iloc[idx].to_dict()
                    level = metadata.get("M·ª©c")

                row = (
                    sheet_name,
                    col_name,
                    idx,
                    text,
                    str(emb),  
                    data_hash,
                    updated_at,
                    level,
                )
                rows.append(row)

        print(f"\nüì¶ Upload {len(rows)} embeddings t·ª´ {file} ({len(embeddings_by_col)} c·ªôt)...")

        if not rows:
            print("‚ö†Ô∏è File kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá, b·ªè qua.")
            continue

        # Upload theo batch 500 b·∫£n ghi/l·∫ßn
        batch_size = 500
        for i in tqdm(range(0, len(rows), batch_size), desc=f"{sheet_name}"):
            chunk = rows[i:i + batch_size]
            try:
                args_str = b",".join(
                    cursor.mogrify("(%s,%s,%s,%s,%s,%s,%s,%s)", row) for row in chunk
                )
                cursor.execute(
                    b"INSERT INTO " + TABLE_NAME.encode() +
                    b" (sheet_name, column_name, row_index, text, embedding, data_hash, updated_at, level) VALUES " +
                    args_str
                )
                conn.commit()
            except Exception as e:
                print("‚ùå L·ªói khi upload batch:", e)
                conn.rollback()

        print(f"‚úÖ Ho√†n t·∫•t upload {file} ({len(rows)} b·∫£n ghi)\n")


def main():
    print("üöÄ B·∫Øt ƒë·∫ßu upload embeddings l√™n PostgreSQL local...")
    upload_embeddings()
    cursor.close()
    conn.close()
    print("üèÅ Ho√†n t·∫•t to√†n b·ªô qu√° tr√¨nh upload!")


if __name__ == "__main__":
    main()
